# -*- coding: utf-8 -*-
"""yt comment analyser preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/194mfHwDLZbmA-qQsxbmPKsod2wVQ2szD
"""

import numpy as np
import pandas as pd

df = pd.read_csv('https://raw.githubusercontent.com/Himanshu-1703/reddit-sentiment-analysis/refs/heads/main/data/reddit.csv')
df.head()

df.shape

df.sample()['clean_comment'].values

df.info()

df.isnull().sum()

df[df['clean_comment'].isna()]

df[df['clean_comment'].isna()]['category'].value_counts()

df.dropna(inplace=True)

df.duplicated().sum()

df[df.duplicated()]

df[(df['clean_comment'].str.strip()=='')]

df=df[~(df['clean_comment'].str.strip()=='')]

df['clean_comment']=df['clean_comment'].str.lower()

df.head()

df[df['clean_comment'].apply(lambda x: x.endswith(' ') or x.startswith(' '))]

df['clean_comment']=df['clean_comment'].str.strip()

df[df['clean_comment'].apply(lambda x: x.endswith(' ') or x.startswith(' '))].sum()

# Identify comments containing URLs
url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
comments_with_urls = df[df['clean_comment'].str.contains(url_pattern, regex=True)]

comments_with_urls

# Identify comments containing new line characters
comments_with_newlines=df[df['clean_comment'].str.contains('\n')]
comments_with_newlines.head()

comments_with_newlines.shape

# Remove new line characters from the 'clean_comment' column
df['clean_comment'] = df['clean_comment'].str.replace('\n', ' ',regex=True)

# Verify the transformation by checking for any remaining new lines
comments_with_newlines_remaining=df[df['clean_comment'].str.contains('\n')]
comments_with_newlines_remaining.head()

"""**EDA**"""

import seaborn as sns
import matplotlib.pyplot as plt

# distribution of classes
sns.countplot(x='category',data=df)

df['category'].value_counts(normalize=True).mul(100).round(2)

df['word_count']=df['clean_comment'].apply(lambda x: len(x.split()))

df.sample(5)

df['word_count'].describe()

sns.displot(df['word_count'],kde=True)

sns.boxplot(df['word_count'])

plt.figure(figsize=(10,6))
sns.boxplot(x='category',y='word_count',data=df)
plt.title('Distribution of word counts by category')
plt.xlabel('Category')
plt.ylabel('Word Count')
plt.show()

plt.figure(figsize=(10,6))

sns.kdeplot(df[df['category']==1]['word_count'],label='positive',fill=True)
sns.kdeplot(df[df['category']==0]['word_count'],label='neutral',fill=True)
sns.kdeplot(df[df['category']==-1]['word_count'],label='negative',fill=True)


plt.title('Word Count Distribution by Category')
plt.xlabel('Word Count')
plt.ylabel('Density')

# Add a legend
plt.legend()

# Show the plot
plt.show()

"""Positive comments (category 1): These tend to have a wider spread in word count, indicating that longer comments are more common in positive sentiments.

Neutral comments (category 0): The distribution shows a relatively lower frequency and is more concentrated around shorter comments compared to positive or negative ones.


Negative comments (category -1): These comments have a distribution somewhat similar to positive comments but with a smaller proportion of longer comments
"""

plt.figure(figsize=(10,6))
sns.scatterplot(data=df,x='category',y='word_count',alpha=0.5)
plt.title('Scatter Plot of Word Count by Category')
plt.xlabel('Category')
plt.ylabel('Word Count')
plt.show()

sns.barplot(df,x='category',y='word_count',estimator='median')
plt.show()

from nltk.corpus import stopwords

import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords

import nltk
nltk.download('stopwords')

stop_words=set(stopwords.words('english'))

df['num_stop_words']=df['clean_comment'].apply(lambda x : len([word for word in x.split() if word in stop_words]))

# Create a distribution plot (displot) for the 'num_stop_words' column
plt.figure(figsize=(10, 6))
sns.histplot(df['num_stop_words'], kde=True)
plt.title('Distribution of Stop Word Count in Comments')
plt.xlabel('Number of Stop Words')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(10,6))

sns.kdeplot(df[df["category"]==1]['num_stop_words'],label="Positive",fill=True)
sns.kdeplot(df[df["category"]==0]['num_stop_words'],label="Neutral",fill=True)
sns.kdeplot(df[df["category"]==-1]['num_stop_words'],label="Negative",fill=True)

plt.title("Num stop words Distribution by Category")
plt.xlabel("Num stop word count")
plt.ylabel('Density')

plt.legend()
plt.show()

sns.barplot(df,x='category',y='num_stop_words',estimator='median')

from collections import Counter


all_stop_words=[word for comment in df['clean_comment'] for word in comment.split() if word in stop_words]

most_common_stop_words=Counter(all_stop_words).most_common(25)

top_25_df=pd.DataFrame(most_common_stop_words,columns=['stop_word','count'])

plt.figure(figsize=(10,6))
sns.barplot(data=top_25_df,x='stop_word',y='count',palette='viridis')
plt.xlabel('Stop Word')
plt.ylabel('Frequency')
plt.title('Top 25 Most Common Stop Words')
plt.xticks(rotation=45)
plt.show()

df['num_chars']=df['clean_comment'].apply(len)

df.sample(10)

df['num_chars'].describe()

from collections import Counter

all_text=' '.join(df['clean_comment'])

char_frequency=Counter(all_text)

char_frequency_df=pd.DataFrame(char_frequency.items(),columns=['character','frequency']).sort_values(by='frequency',ascending=True)

char_frequency_df['character'].values

char_frequency_df.tail(50)

df['num_punctuation_char']=df['clean_comment'].apply(lambda x: sum([1 for char in x if char in'.,!?;:"\'()[]{}-'
]))

df.head()

df['num_punctuation_char'].describe()

from sklearn.feature_extraction.text import CountVectorizer

# Create a function to extract the top 25 bigrams
def get_top_ngrams(corpus, n=None):
    vec = CountVectorizer(ngram_range=(2, 2), stop_words='english').fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0)
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)
    return words_freq[:n]

# Get the top 25 bigrams
top_25_bigrams = get_top_ngrams(df['clean_comment'], 25)

# Convert the bigrams into a DataFrame for plotting
top_25_bigrams_df = pd.DataFrame(top_25_bigrams, columns=['bigram', 'count'])

# Plot the countplot for the top 25 bigrams
plt.figure(figsize=(12, 8))
sns.barplot(data=top_25_bigrams_df, x='count', y='bigram', palette='magma')
plt.title('Top 25 Most Common Bigrams')
plt.xlabel('Count')
plt.ylabel('Bigram')
plt.show()

# Create a function to extract the top 25 trigrams
def get_top_trigrams(corpus, n=None):
    vec = CountVectorizer(ngram_range=(3, 3), stop_words='english').fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0)
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)
    return words_freq[:n]

# Get the top 25 trigrams
top_25_trigrams = get_top_trigrams(df['clean_comment'], 25)

# Convert the trigrams into a DataFrame for plotting
top_25_trigrams_df = pd.DataFrame(top_25_trigrams, columns=['trigram', 'count'])

# Plot the countplot for the top 25 trigrams
plt.figure(figsize=(12, 8))
sns.barplot(data=top_25_trigrams_df, x='count', y='trigram', palette='coolwarm')
plt.title('Top 25 Most Common Trigrams')
plt.xlabel('Count')
plt.ylabel('Trigram')
plt.show()

import re

df['clean_comment']=df['clean_comment'].apply(lambda x: re.sub(r'[^A-Za-z0-9\s!?.,]','',str(x)) )

all_text = ' '.join(df['clean_comment'])

# Count the frequency of each character
char_frequency = Counter(all_text)

# Convert the character frequency into a DataFrame for better display
char_frequency_df = pd.DataFrame(char_frequency.items(), columns=['character', 'frequency']).sort_values(by='frequency', ascending=False)

df.head()

from matplotlib import pyplot as plt
df['word_count'].plot(kind='hist', bins=20, title='word_count')
plt.gca().spines[['top', 'right',]].set_visible(False)

from nltk.corpus import stopwords

stop_words=set(stopwords.words('english'))-{'not',"however","no",'yet'}

df['clean_comment']=df['clean_comment'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words]))

df.head()

from nltk.stem import WordNetLemmatizer

nltk.download('wordnet')

lemmatizer=WordNetLemmatizer()

df['clean_comment']=df['clean_comment'].apply(lambda x : ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))

df.head()

from wordcloud import WordCloud
import matplotlib.pyplot as plt

def plot_word_cloud(text):
  wordcloud=WordCloud(width=800,height=400,background_color='white').generate(' '.join(text))
  plt.figure(figsize=(10, 5))
  plt.imshow(wordcloud, interpolation='bilinear')
  plt.axis("off")
  plt.show()

plot_word_cloud(df['clean_comment'])

from wordcloud import WordCloud
import matplotlib.pyplot as plt

def plot_word_cloud(text):
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(text))
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.show()

plot_word_cloud(df[df['category'] == 1]['clean_comment'])

from wordcloud import WordCloud
import matplotlib.pyplot as plt

def plot_word_cloud(text):
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(text))
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.show()

plot_word_cloud(df[df['category'] == 0]['clean_comment'])

from wordcloud import WordCloud
import matplotlib.pyplot as plt

def plot_word_cloud(text):
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(text))
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.show()

plot_word_cloud(df[df['category'] == -1]['clean_comment'])

def plot_top_n_words(df, n=20):
    """Plot the top N most frequent words in the dataset."""
    # Flatten all words in the content column
    words = ' '.join(df['clean_comment']).split()

    # Get the top N most common words
    counter = Counter(words)
    most_common_words = counter.most_common(n)

    # Split the words and their counts for plotting
    words, counts = zip(*most_common_words)

    # Plot the top N words
    plt.figure(figsize=(10, 6))
    sns.barplot(x=list(counts), y=list(words))
    plt.title(f'Top {n} Most Frequent Words')
    plt.xlabel('Frequency')
    plt.ylabel('Words')
    plt.show()

# Example usage
plot_top_n_words(df, n=50)

